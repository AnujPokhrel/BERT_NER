Fast adaptive interpolation of multi-dimensional arrays in tensor train format. Using recently proposed tensor train format for the representation of multi-dimensional dense arrays (tensors) we develop a fast interpolation method to approximate the given tensor by using only a small number of its elements. The algorithm is based on DMRG scheme, known among the quantum chemistry society. It is modified to make an interpolation on the adaptive set of tensor elements. The latter is selected using the maximum-volume principle which was previously used for the cross approximation schemes for matrices and 3-tensors. The numerical examples include the interpolation of one– and many–dimensional functions on the uniform grids.

Iterative representing set selection for nested cross approximation. A new fast algebraic method for obtaining an H2-approximation of a matrix from its entries is presented. The main idea behind the method is based on the nested representation and the maximum volume principle to select submatrices in low-rank matrices. A special iterative approach for the computation of so-called representing sets is established. The main advantage of the method is that it uses only the hierarchical partitioning of the matrix and does not require special ‘proxy surfaces’ to be selected in advance. The numerical experiments for the electrostatic problem and for the boundary integral operator confirm the effectiveness and robustness of the approach. The complexity is linear in the matrix size and polynomial in the ranks. The algorithm is implemented as an open-source Python package that is available online

Incomplete Cross Approximation in the Mosaic-Skeleton Method. The mosaic-skeleton method was bred in a simple observation that rather large blocks in very large matrices coming from integral formulations can be approximated accurately by a sum of just few rank-one matrices (skeletons). These blocks might correspond to a region where the kernel is smooth enough, and anyway it can be a region where the kernel is approximated by a short sum of separable functions (functional skeletons). Since the effect of approximations is like that of having small-rank matrices, we find it pertinent to say about mosaic ranks of a matrix which turn out to be pretty small for many nonsingular matrices.

Black Box Low Tensor-Rank Approximation Using Fiber-Crosses. In this article we introduce a black box type algorithm for the approximation of tensors A in high dimension d. The algorithm adaptively determines the positions of entries of the tensor that have to be computed or read, and using these (few) entries it constructs a low rank tensor approximation X that minimizes the 2-distance between A and X at the chosen positions. The full tensor A is not required, only the evaluation of A at a few positions. The minimization problem is solved by Newton’s method, which requires the computation and evaluation of the Hessian. For efficiency reasons the positions are located on fiber-crosses of the tensor so that the Hessian can be assembled and evaluated in a data-sparse form requiring a complexity of O(P d), where P is the number of fiber-crosses and d the order of the tensor.

How to Find a Good Submatrix. Pseudoskeleton approximation and some other problems require the knowledge of sufficiently well-conditioned submatrix in a large scale matrix. The quality of a submatrix can be measured by modulus of its determinant, also known as volume. In this paper we discuss a search algorithm for the maximum-volume submatrix which already proved to be useful in several matrix and tensor approximation algorithms. We investigate the behavior of this algorithm on random matrices and present some of its applications, including maximization of a bivariate functional.

Exponential Machines. Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g., recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K.

Scalable Gaussian Processes with Billions of Inducing Inputs via Tensor Train Decomposition. We propose a method (TT-GP) for approximate inference in Gaussian Process (GP) models. We build on previous scalable GP research including stochastic variational inference based on inducing inputs, kernel interpolation, and structure exploiting algebra. The key idea of our method is to use Tensor Train decomposition for variational parameters, which allows us to train GPs with billions of inducing inputs and achieve state of-the-art results on several benchmarks. Further, our approach allows for training kernels based on deep neural networks without any modifications to the underlying GP model. A neural network learns a multidimensional embedding for the data, which is used by the GP to make the final prediction, without pretraining, through maximization of GP marginal likelihood. We show the efficiency of the proposed approach on several regression and classification benchmark datasets including MNIST, CIFAR-10, and Airline.

O(d logN)-Quantics Approximation of N-d Tensors in High-Dimensional Numerical Modeling. In the present paper, we discuss the novel concept of super-compressed tensor-structured data formats in high-dimensional applications. We describe the multifolding or quantics-based tensor approximation method of O(d logN)-complexity (logarithmic scaling in the volume size), applied to the discrete functions over the product index set {1, . . . , N}⊗^d , or briefly N-d tensors of size N^d , and to the respective discretized differential-integral operators in R^d . As the basic approximation result, we prove that a complex exponential sampled on an equispaced grid has quantics rank 1. Moreover, a Chebyshev polynomial, sampled over a Chebyshev Gauss–Lobatto grid, has separation rank 2 in the quantics tensor format, while for the polynomial of degree mover a Chebyshev grid the respective quantics rank is at most 2m+1. For N-d tensors generated by certain analytic functions, we give a constructive proof of the O(d logN log ε−1)-complexity bound for their approximation by low-rank 2-(d logN) quantics tensors up to the accuracy ε > 0. In the case ε = O(N−α), α > 0, our approach leads to the quantics tensor numerical method in dimension d, with the nearly optimal asymptotic complexity O(d/α log^2 ε −1). From numerical examples presented here, we observe that the quantics tensor method has proved its value in application to various function related tensors/matrices arising in computational quantum chemistry and in the traditional finite element method/boundary element method (FEM/BEM). The tool apparently works.

Superfast Fourier Transform Using QTT Approximation. We propose Fourier transform algorithms using QTT format for data sparse approximate representation of one- and multi-dimensional vectors (m-tensors). Although the Fourier matrix itself does not have a low-rank QTT representation, it can be efficiently applied to a vector in the QTT format exploiting the multilevel structure of the Cooley-Tukey algorithm. The m-dimensional Fourier transform of an n x… x n vector with n = 2^d has O(md^2R^3) complexity, where R is the maximum QTT-rank of input, output and all intermediate vectors in the procedure. For the vectors with moderate R and large n and m the proposed algorithm outperforms the O(nm log n) fast Fourier transform (FFT) algorithm and has asymptotically the same log-squared complexity as the superfast quantum Fourier transform (QFT) algorithm. By numerical experiments we demonstrate the examples of problems for which the use of QTT format relaxes the grid size constrains and allows the high-resolution computations of Fourier images and convolutions in higher dimensions without the ‘curse of dimensionality’. We compare the proposed method with Sparse Fourier transform algorithms and show that our approach is competitive for signals with small number of randomly distributed frequencies and signals with limited bandwidth.

Expanding the Range of Hierarchical Equations of Motion by Tensor-Train Implementation. The non-equilibrium thermo-field dynamics formulation of the hierarchical equations of motion combined with the tensor-train representation of the density matrix is discussed, and a new numerical integration scheme is introduced. The numerical methodology is based on an adaptive low-rank Galerkin reduction scheme and can preserve linear invariants (such as the trace of the density matrix). The method is applied to the study of the charge transfer dynamics in model pentacene molecular aggregates. The combined effect of a discrete set of molecular vibrational modes and a thermal bath is investigated, paying special attention to the coherent−incoherent transition of the charge transport. The new computational framework is shown to be a very promising methodology for the study of the quantum dynamics of complex molecular systems in the condensed phase.

Solving the time-independent Schrodinger equation for chains of coupled excitons and phonons using tensor trains. We demonstrate how to apply the tensor-train format to solve the time-independent Schrodinger equation for quasi one-dimensional excitonic chain systems with and without periodic boundary conditions. The coupled excitons and phonons are modeled by Frenkel-Holstein type Hamiltonians with on-site and nearest-neighbor interactions only. We reduce the memory consumption as well as the computational costs significantly by employing efficient decompositions to construct low rank tensor-train representations, thus mitigating the curse of dimensionality. In order to compute also higher quantum states, we introduce an approach which directly incorporates the Wielandt deflation technique into the alternating linear scheme for the solution of eigenproblems. Besides systems with coupled excitons and phonons, we also investigate uncoupled problems for which (semi-)analytical results exist. There, we find that in case of homogeneous systems the tensor train ranks of state vectors only marginally depend on the chain length which results in a linear growth of the storage consumption. However, the CPU time increases slightly faster with the chain length than the storage consumption because the alternating linear scheme adopted in our work requires more iterations to achieve convergence for longer chains and a given rank. Finally, we demonstrate that the tensor-train approach to the quantum treatment of coupled excitons and phonons makes it possible to directly tackle the phenomenon of mutual self-trapping. We are able to con rm the main results of the Davydov theory, i.e., the dependence of the wavepacket width and the corresponding stabilization energy on the exciton-phonon coupling strength, though only for a certain range of that parameter. In future work, our approach will allow calculations also beyond the validity regime of that theory and/or beyond the restrictions of the Frenkel-Holstein type Hamiltonians.

Solution decomposition for the nonlinear Poisson-Boltzmann equation using the range-separated tensor format. The Poisson-Boltzmann equation (PBE) is an implicit solvent continuum model for calculating the electrostatic potential and energies of ionic solvated biomolecules. However, its numerical solution remains a significant challenge due strong singularities and nonlinearity caused by the singular source terms and the exponential nonlinear terms, respectively. An efficient method for the treatment of singularities in the linear PBE was introduced in [1], that is based on the RS tensor decomposition [2] for both electrostatic potential and the discretized Dirac delta distribution [3]. In this paper, we extend this regularization method to the nonlinear PBE. Similar to [1] we apply the PBE only to the regular part of the solution corresponding to the modified right-hand side via extraction of the long-range part in the discretized Dirac delta distribution. The total electrostatic potential is obtained by adding the long-range solution to the directly precomputed short-range potential. The main computational benefit of the approach is the automatic maintaining of the continuity in the Cauchy data on the solute-solvent interface. The boundary conditions are also obtained from the long-range component of the precomputed canonical tensor representation of the Newton kernel. In the numerical experiments, we illustrate the accuracy of the nonlinear regularized PBE (NRPBE) over the classical variant.

Automatic Differentiation for Riemannian Optimization on Low-Rank Matrix and Tensor-Train Manifolds. In scientific computing and machine learning applications, matrices and more general multidimensional arrays (tensors) can often be approximated with the help of low-rank decompositions. Since matrices and tensors of fixed rank form smooth Riemannian manifolds, one of the popular tools for finding low-rank approximations is to use Riemannian optimization. Nevertheless, efficient implementation of Riemannian gradients and Hessians, required in Riemannian optimization algorithms, can be a nontrivial task in practice. Moreover, in some cases, analytic formulas are not even available. In this paper, we build upon automatic differentiation and propose a method that, given an implementation of the function to be minimized, efficiently computes Riemannian gradients and matrix-by-vector products between an approximate Riemannian Hessian and a given vector.

Putting MRFs on a Tensor Train. In the paper we present a new framework for dealing with probabilistic graphical models. Our approach relies on the recently proposed Tensor Train format (TT-format) of a tensor that while being compact allows for efficient application of linear algebra operations. We present a way to convert the energy of a Markov random field to the TT-format and show how one can exploit the properties of the TT-format to attack the tasks of the partition function estimation and the MAP-inference. We provide theoretical guarantees on the accuracy of the proposed algorithm for estimating the partition function and compare our methods against several state-of-the-art algorithms.

TT-cross approximation for multidimensional arrays. As is well known, a rank-r matrix can be recovered from a cross of r linearly independent columns and rows, and an arbitrary matrix can be interpolated on the cross entries. Other entries by this cross or pseudo-skeleton approximation are given with errors depending on the closeness of the matrix to a rank-r matrix and as well on the choice of cross. In this paper we extend this construction to d-dimensional arrays (tensors) and suggest a new interpolation formula in which a d-dimensional array is interpolated on the entries of some TT-cross (tensor-train-cross). The total number of entries and the complexity of our interpolation algorithm depend on d linearly, so the approach does not suffer from the curse of dimensionality. We also propose a TT-cross method for computation of d dimensional integrals and apply it to some examples with dimensionality in the range from d = 100 up to d = 4000 and the relative accuracy of order 10^(−10). In all constructions we capitalize on the new tensor decomposition in the form of tensor trains (TT-decomposition).

Rectangular maximum-volume submatrices and their applications. We introduce a definition of the volume of a general rectangular matrix, which is equivalent to an absolute value of the determinant for square matrices. We generalize results of square maximum-volume submatrices to the rectangular case, show a connection of the rectangular volume with an optimal experimental design and provide estimates for a growth of coefficients and an approximation error in spectral and Chebyshev norms. Three promising applications of such submatrices are presented: recommender systems, finding maximal elements in low-rank matrices and preconditioning of overdetermined linear systems. The code is available online.

Post-Processing of High-Dimensional Data. Scientific computations or measurements may result in huge volumes of high dimensional data, for instance 1020 or 100300 elements. Often these can be thought of representing a real-valued function on a high-dimensional domain, and can be conceptually arranged in the format of a tensor of high degree in some truncated or lossy compressed format. We look at some common post-processing tasks which are too time and storage consuming in the uncompressed data format and not obvious in the compressed format, as such huge data sets cannot be stored in their entirety, and the value of an element is not readily accessible through simple look-up. The tasks we consider are finding the location of maximum or minimum, or minimum and maximum of a function of the data, or finding the indices of all elements in some interval — i.e. level sets, the number of elements with a value in such a level set, the probability of an element being in a particular level set, and the mean and variance of the total collection. The algorithms to be described are fixed point iterations of a particular function of the tensor, which will then exhibit the desired result. For this, the data is considered as an element of a high degree tensor space, although in an abstract sense, the algorithms are independent of the representation of the data as a tensor. All that we require is: the data are given in a compressed data format, and the rank truncation procedure preserves compression. We allow the actual computational representation to be a lossy compression, and we allow the algebra operations to be performed in an approximate fashion, so as to maintain a high compression level. One such example which we address explicitly is the representation of data as a tensor with compression in the form of a low-rank representation.

Approximation of boundary element matrices. This article considers the problem of approximating a general asymptotically smooth function in two variables, typically arising in integral formulations of boundary value problems, by a sum of products of two functions in one variable. From these results an iterative algorithm for the low-rank approximation of blocks of large unstructured matrices generated by asymptotically smooth functions is developed. This algorithm uses only few entries from the original block and since it has a natural stopping criterion the approximative rank is not needed in advance.

A Theory of Pseudoskeleton Approximations. Let an m X n matrix A be approximated by a rank-r matrix with an accuracy E. We prove that it is possible to choose r columns and r rows of A forming a so-called pseudoskeleton component which approximates A with O(eps*sqrt(r)*(sqrt(m) + sqrt(n))) accuracy in the sense of the 2-norm. On the way to this estimate we study the interconnection between the volume (i.e., the determinant in the absolute value) and the minimal singular value sig_r of T x r submatrices of an n x r matrix with orthogonal columns. We propose a lower bound (better than one given by Chandrasekaran and Ipsen and by Hong and Pan) for the maximum of sig_r, over all these submatrices and formulate a hypothesis on a tighter bound. 


Hybrid Symbolic-Numeric Integration in Multiple Dimensions via Tensor-Product Series. We present a new hybrid symbolic-numeric method for the fast and accurate evaluation of definite integrals in multiple dimensions. This method is well-suited for two classes of problems: (1) analytic integrands over general regions in two dimensions, and (2) families of analytic integrands with special algebraic structure over hyper-rectangular regions in higher dimensions. The algebraic theory of multivariate interpolation via natural tensor product series was developed in the doctoral thesis by Chapman, who named this broad new scheme of bilinear series expansion ”Geddes series” in honour of his thesis supervisor. This paper describes an efficient adaptive algorithm for generating bilinear series of Geddes-Newton type and explores applications of this algorithm to multiple integration. We will present test results demonstrating that our new adaptive integration algorithm is effective both in high dimensions and with high accuracy. For example, our Maple implementation of the algorithm has successfully computed nontrivial integrals with hundreds of dimensions to 10-digit accuracy, each in under 3 minutes on a desktop computer. Current numerical multiple integration methods either become very slow or yield only low accuracy in high dimensions, due to the necessity to sample the integrand at a very large number of points. Our approach overcomes this difficulty by using a Geddes-Newton series with a modest number of terms to construct an accurate tensor-product approximation of the integrand. The partial separation of variables achieved in this way reduces the original integral to a manageable bilinear combination of integrals of essentially half the original dimension. We continue halving the dimensions recursively until obtaining one-dimensional integrals, which are then computed by standard numeric or symbolic techniques.

Simultaneous state-time approximation of the chemical master equation using tensor product formats. We study the application of the novel tensor formats (TT, QTT, QTT-Tucker) to the solution of d-dimensional chemical master equations, applied mostly to gene regulating networks (signaling cascades, toggle switches, phage-lambda). For some important cases, e.g. signaling cascade models, we prove good separability properties of the system operator. The Quantized tensor representations (QTT, QTT-Tucker) are employed in both state space and time, and the global state-time (d + 1)-dimensional system is solved in the structured form by using the ALS-type iteration. This approach leads to the logarithmic dependence of the computational complexity on the system size. When possible, we compare our approach with the direct CME solution and some previously known approximate schemes, and observe a good potential of the newer tensor methods in simulation of relevant biological systems.

Computation of the Response Surface in the Tensor Train data format. We apply the Tensor Train (TT) approximation to construct the Polynomial Chaos Expansion (PCE) of a random field, and solve the stochastic elliptic diffusion PDE with the stochastic Galerkin discretization. We compare two strategies of the polynomial chaos expansion: sparse and full polynomial (multi-index) sets. In the full set, the polynomial orders are chosen independently in each variable, which provides higher flexibility and accuracy. However, the total amount of degrees of freedom grows exponentially with the number of stochastic coordinates. To cope with this curse of dimensionality, the data is kept compressed in the TT decomposition, a recurrent low-rank factorization. PCE computations on sparse grids sets are extensively studied, but the TT representation for PCE is a novel approach that is investigated in this paper. We outline how to deduce the PCE from the covariance matrix, assemble the Galerkin operator, and evaluate some post-processing (mean, variance, Sobol indices), staying within the low-rank framework. The most demanding are two stages. First, we interpolate PCE coefficients in the TT format using a few number of samples, which is performed via the block cross approximation method. Second, we solve the discretized equation (large linear system) via the alternating minimal energy algorithm. In the numerical experiments we demonstrate that the full expansion set encapsulated in the TT format is indeed preferable in cases when high accuracy and high polynomial orders are required.

Tensor Numerical Methods for High-dimensional PDEs: Basic Theory and Initial Applications. We present a brief survey on the modern tensor numerical methods for multidimensional stationery and time-dependent partial differential equations (PDEs). The guiding principle of the tensor approach is the rank-structured separable approximation of multivariate functions and operators represented on a grid. Recently, the traditional Tucker, canonical, and matrix product states (tensor train) tensor models have been applied to the grid-based electronic structure calculations, to parametric PDEs, and to dynamical equations arising in scientific computing. The essential progress is based on the quantics tensor approximation method proved to be capable to represent (approximate) function related d-dimensional data arrays of size Nd with log-volume complexity, O(d logN). Combined with the traditional numerical schemes, these novel tools establish a new promising approach for solving multidimensional integral and differential equations using low-parametric rank-structured tensor formats. As the main example, we describe the grid-based tensor numerical approach for solving the 3D nonlinear Hartree-Fock eigenvalue problem, that was the starting point for the developments of tensor-structured numerical methods for large-scale computations in solving real-life multidimensional problems. We also discuss a new method for the fast 3D lattice summation of electrostatic potentials by assembled low-rank tensor approximation capable to treat the potential sum over millions of atoms in few seconds. We address new results on tensor approximation of the dynamical Fokker-Planck and master equations in many dimensions up to d = 20. Numerical tests demonstrate the benefits of the rank-structured tensor approximation on the forementioned examples of multidimensional PDEs. In particular, the use of grid-based tensor representations in the reduced basis of atomics orbitals yields an accurate solution of the Hartree-Fock equation on large N x … x N grids with a grid size of up to N = 105.

Polynomial Chaos Expansion of random coefficients and the solution of stochastic partial differential equations in the Tensor Train format. We apply the Tensor Train (TT) decomposition to construct the tensor product Polynomial Chaos Expansion (PCE) of a random field, to solve the stochastic elliptic diffusion PDE with the stochastic Galerkin discretization, and to compute some quantities of interest (mean, variance, exceedance probabilities). We assume that the random diffusion coefficient is given as a smooth transformation of a Gaussian random field. In this case, the PCE is delivered by a complicated formula, which lacks an analytic TT representation. To construct its TT approximation numerically, we develop the new block TT cross algorithm, a method that computes the whole TT decomposition from a few evaluations of the PCE formula. The new method is conceptually similar to the adaptive cross approximation in the TT format, but is more efficient when several tensors must be stored in the same TT representation, which is the case for the PCE. Besides, we demonstrate how to assemble the stochastic Galerkin matrix and to compute the solution of the elliptic equation and its post-processing, staying in the TT format. We compare our technique with the traditional sparse polynomial chaos and the Monte Carlo approaches. In the tensor product polynomial chaos, the polynomial degree is bounded for each random variable independently. This provides higher accuracy than the sparse polynomial set or the Monte Carlo method, but the cardinality of the tensor product set grows exponentially with the number of random variables. However, when the PCE coefficients are implicitly approximated in the TT format, the computations with the full tensor product polynomial set become possible. In the numerical experiments, we confirm that the new methodology is competitive in a wide range of parameters, especially where high accuracy and high polynomial degrees are required.

A hybrid Alternating Least Squares – TT Cross algorithm for parametric PDEs. We consider the approximate solution of parametric PDEs using the low-rank Tensor Train (TT) decomposition. Such parametric PDEs arise for example in uncertainty quantification problems in engineering applications. We propose an algorithm that is a hybrid of the alternating least squares and the TT cross methods. It computes a TT approximation of the whole solution, which is beneficial when multiple quantities of interest are sought. This might be needed, for example, for the computation of the probability density function (PDF) via the maximum entropy method [Kavehrad and Joseph, IEEE Trans. Comm., 1986]. The new algorithm exploits and preserves the block diagonal structure of the discretized operator in stochastic collocation schemes. This disentangles computations of the spatial and parametric degrees of freedom in the TT representation. In particular, it only requires solving independent PDEs at a few parameter values, thus allowing the use of existing high performance PDE solvers. In our numerical experiments, we apply the new algorithm to the stochastic diffusion equation and compare it with preconditioned steepest descent in the TT format, as well as with (multilevel) quasi-Monte Carlo and dimension-adaptive sparse grids methods. For sufficiently smooth random fields the new approach is orders of magnitude faster.

Kriging in Tensor Train data format. Combination of low-tensor rank techniques and the Fast Fourier transform (FFT) based methods had turned out to be prominent in accelerating various statistical operations such as Kriging, computing conditional covariance, geostatistical optimal design, and others. However, the approximation of a full tensor by its low-rank format can be computationally formidable. In this work, we incorporate the robust Tensor Train (TT) approximation of covariance matrices and the efficient TT-Cross algorithm into the FFT-based Kriging. It is shown that here the computational complexity of Kriging is reduced to O(dr3n), where n is the mode size of the estimation grid, d is the number of variables (the dimension), and r is the rank of the TT approximation of the covariance matrix. For many popular covariance functions the TT rank r remains stable for increasing n and d. The advantages of this approach against those using plain FFT are demonstrated in synthetic and real data examples.

Solving Differential Riccati Equations: A Nonlinear Space-Time Method Using Tensor Trains. Differential algebraic Riccati equations are at the heart of many applications in control theory. They are time-depent, matrix-valued, and in particular nonlinear equations that require special methods for their solution. Low-rank methods have been used heavily computing a low-rank solution at every step of a time-discretization. We propose the use of an all-at-once space-time solution leading to a large nonlinear space-time problem for which we propose the use of a Newton–Kleinman iteration. Approximating the space-time problem in low-rank form requires fewer applications of the discretized differential operator and gives a low-rank approximation to the overall solution.

A low-rank solver for the stochastic unsteady Navier–Stokes problem. We study a low-rank iterative solver for the unsteady Navier–Stokes equations for incompressible flows with a stochastic viscosity. The equations are discretized using the stochastic Galerkin method, and we consider an all-at-once formulation where the algebraic systems at all the time steps are collected and solved simultaneously. The problem is linearized with Picard’s method. To efficiently solve the linear systems at each step, we use low-rank tensor representations within the Krylov subspace method, which leads to significant reductions in storage requirements and computational costs. Combined with effective mean based preconditioners and the idea of inexact solve, we show that only a small number of linear iterations are needed at each Picard step. The proposed algorithm is tested with a model of flow in a two-dimensional symmetric step domain with different settings to demonstrate the computational efficiency.

Guaranteed a posteriori error bounds for low rank tensor approximate solutions. We propose a guaranteed and fully computable upper bound on the energy norm of the error in low-rank Tensor Train (TT) approximate solutions of (possibly) high dimensional reaction-diffusion problems. The error bound is obtained from Euler–Lagrange equations for a complementary flux reconstruction problem, which are solved in the low-rank TT representation using the block Alternating Linear Scheme. This bound is guaranteed to be above the energy norm of the total error, including the discretization error, the tensor approximation error, and the error in the solver of linear algebraic equations, although quadrature errors, in general, can pollute its evaluation. Numerical examples with the Poisson equation and the Schrodinger equation with the Henon-Heiles potential in up to 40 dimensions are presented to illustrate the efficiency of this approach.

Low-rank solution of an optimal control problem constrained by random Navier-Stokes equations. We develop a low-rank tensor decomposition algorithm for the numerical solution of a distributed optimal control problem constrained by two-dimensional time-dependent Navier-Stokes equations with a stochastic inflow. The goal of optimization is to minimize the flow vorticity. The inflow boundary condition is assumed to be an infinite-dimensional random field, which is parametrized using a finite- (but high-) dimensional Fourier expansion and discretized using the stochastic Galerkin finite element method. This leads to a prohibitively large number of degrees of freedom in the discrete solution. Moreover, the optimality conditions in a time-dependent problem require solving a coupled saddle-point system of nonlinear equations on all time steps at once. For the resulting discrete problem, we approximate the solution by the tensor-train (TT) decomposition and propose a numerically efficient algorithm to solve the optimality equations directly in the TT representation. This algorithm is based on the alternating linear scheme (ALS), but in contrast to the basic ALS method, the new algorithm exploits and preserves the block structure of the optimality equations. We prove that this structure preservation renders the proposed block ALS method well posed, in the sense that each step requires the solution of a nonsingular reduced linear system, which might not be the case for the basic ALS. Finally, we present numerical experiments based on two benchmark problems of simulation of a flow around a von Kármán vortex and a backward step, each of which has uncertain inflow. The experiments demonstrate a significant complexity reduction achieved using the TT representation and the block ALS algorithm. Specifically, we observe that the high-dimensional stochastic time-dependent problem can be solved with the asymptotic complexity of the corresponding deterministic problem.

Solving high-dimensional parabolic PDEs using the tensor train format. High-dimensional partial differential equations (PDEs) are ubiquitous in economics, science and engineering. However, their numerical treatment poses formidable challenges since traditional grid based methods tend to be frustrated by the curse of dimensionality. In this paper, we argue that tensor trains provide an appealing approximation framework for parabolic PDEs: the combination of reformulations in terms of backward stochastic differential equations and regression-type methods in the tensor format holds the promise of leveraging latent low-rank structures enabling both compression and efficient computation. Following this paradigm, we develop novel iterative schemes, involving either explicit and fast or implicit and accurate updates. We demonstrate in a number of examples that our methods achieve a favorable trade-off between accuracy and computational efficiency in comparison with state-of-the-art neural network based approaches.

Tensor Decomposition Methods for High-Dimensional Hamilton--Jacobi--Bellman Equations. A tensor decomposition approach for the solution of high-dimensional, fully nonlinear Hamilton--Jacobi--Bellman equations arising in optimal feedback control of nonlinear dynamics is presented. The method combines a tensor train approximation for the value function together with a Newton-like iterative method for the solution of the resulting nonlinear system. The tensor approximation leads to a polynomial scaling with respect to the dimension, partially circumventing the curse of dimensionality. A convergence analysis for the linear-quadratic case is presented. For nonlinear dynamics, the effectiveness of the high-dimensional control synthesis method is assessed in the optimal feedback stabilization of the Allen--Cahn and Fokker--Planck equations with a hundred of variables.

TT-GMRES: on solution to a linear system in the structured tensor format. A adapted tensor-structured GMRES method for the TT format is proposed and investigated. The Tensor Train (TT) approximation is a robust approach to high dimensional problems. One class of problems is solution of a linear system. In this work we study the convergence of the GMRES method in the presence of tensor approximations and provide relaxation techniques to improve its performance. Several numerical examples are presented. The method is also compared with a projection TT linear solver based on the ALS and DMRG methods. On a particular sPDE (high-dimensional parametric) problem, these methods manifest comparable performance, with a good preconditioner the TT-GMRES overcomes the ALS solver.


TTRISK: Tensor Train Decomposition Algorithm for Risk Averse Optimization. This article develops a new algorithm named TTRISK to solve high-dimensional risk-averse optimization problems governed by differential equations (ODEs and/or PDEs) under uncertainty. As an example, we focus on the so-called Conditional Value at Risk (CVaR), but the approach is equally applicable to other coherent risk measures. Both the full and reduced space formulations are considered. The algorithm is based on low rank tensor approximations of random fields discretized using stochastic collocation. To avoid non-smoothness of the objective function underpinning the CVaR, we propose an adaptive strategy to select the width parameter of the smoothed CVaR to balance the smoothing and tensor approximation errors. Moreover, unbiased Monte Carlo CVaR estimate can be computed by using the smoothed CVaR as a control variate. To accelerate the computations, we introduce an efficient preconditioner for the KKT system in the full space formulation. The numerical experiments demonstrate that the proposed method enables accurate CVaR optimization constrained by large-scale discretized systems. In particular, the first example consists of an elliptic PDE with random coefficients as constraints. The second example is motivated by a realistic application to devise a lockdown plan for United Kingdom under COVID-19. The results indicate that the risk-averse framework is feasible with the tensor approximations under tens of random variables.

Tensor train rank minimization with nonlocal self-similarity for tensor completion. The tensor train (TT) rank has received increasing attention in tensor completion due to its ability to capture the global correlation of high-order tensors (order>3). For third order visual data, direct TT rank minimization has not exploited the potential of TT rank for high-order tensors. The TT rank minimization accompany with ket augmentation, which transforms a lower-order tensor (e.g., visual data) into a higher-order tensor, suffers from serious block-artifacts. To tackle this issue, we suggest the TT rank minimization with nonlocal self-similarity for tensor completion by simultaneously exploring the spatial, temporal/spectral, and nonlocal redundancy in visual data. More precisely, the TT rank minimization is performed on a formed higher-order tensor called group by stacking similar cubes, which naturally and fully takes advantage of the ability of TT rank for high-order tensors. Moreover, the perturbation analysis for the TT low-rankness of each group is established. We develop the alternating direction method of multipliers tailored for the specific structure to solve the proposed model. Extensive experiments demonstrate that the proposed method is superior to several existing state-of-the-art methods in terms of both qualitative and quantitative measures. 

Tensor Networks and Hierarchical Tensors for the Solution of High-dimensional Partial Differential Equations. Hierarchical tensors can be regarded as a generalization, preserving many crucial features, of the singular value decomposition to higher-order tensors. For a given tensor product space, a recursive decomposition of the set of coordinates into a dimension tree gives a hierarchy of nested subspaces and corresponding nested bases. The dimensions of these subspaces yield a notion of multilinear rank. This rank tuple, as well as quasi-optimal low-rank approximations by rank truncation, can be obtained by a hierarchical singular value decomposition. For fixed multilinear ranks, the storage and operation complexity of these hierarchical representations scale only linearly in the order of the tensor. As in the matrix case, the set of hierarchical tensors of a given multilinear rank is not a convex set, but forms an open smooth manifold. A number of techniques for the computation of hierarchical low-rank approximations have been developed, including local optimization techniques on Riemannian manifolds as well as truncated iteration methods, which can be applied for solving high-dimensional partial differential equations. This article gives a survey of these developments. We also discuss applications to problems in uncertainty quantification, to the solution of the electronic Schrodinger equation in the strongly correlated regime, and to the computation of metastable states in molecular dynamics.

Tensor-Train Decomposition. A simple non-recursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator. 

Tensor-Train Format Solution with Preconditioned Iterative Method for High Dimensional Time-Dependent Space-Fractional Diffusion Equations with Error Analysis. In this paper, a first order implicit finite difference scheme with Krylov subspace linear system solver is employed to solving time-dependent space-fractional diffusion equations in high dimensions where the initial condition and source term are in tensor-train (TT) format with low TT-ranks. In the time-marching process, TT-format of the solution is maintained and the increment of TT-ranks due to addition is moderated by rounding. The error introduced by rounding is shown to be consistent with the first order finite difference scheme. On the other hand, the linear systems involved in the solution process are shown to possess Toeplitz-like structure so that the complexity and required memory for Krylov subspace solver can be optimized. Further reduction in complexity is made by utilizing a circulant preconditioner which accelerates the convergence rate of Krylov subspace method dramatically. Numerical examples for problems up to 20 dimensions are presented.

On the approximation of high-dimensional differential equations in the hierarchical Tucker format. The hierarchical Tucker format is a way to decompose a high-dimensional tensor recursively into sums of products of lower-dimensional tensors. The number of degrees of freedom in such a representation is typically many orders of magnitude lower than the number of entries of the original tensor. This makes the hierarchical Tucker format a promising approach to solve ordinary differential equations for high-dimensional tensors. In order to propagate the approximation in time, differential equations for the parameters of the hierarchical Tucker format are derived from the Dirac-Frenkel variational principle. We prove an error bound for the dynamical approximation in the hierarchical Tucker format by extending previous results of Koch and Lubich for the non-hierarchical Tucker format.

A semi-Lagrangian Vlasov solver in tensor train format. In this article, we derive a semi-Lagrangian scheme for the solution of the Vlasov equation represented as a low-parametric tensor. Grid-based methods for the Vlasov equation have been shown to give accurate results but their use has mostly been limited to simulations in two dimensional phase space due to extensive memory requirements in higher dimensions. Compression of the solution via high-order singular value decomposition can help in reducing the storage requirements and the tensor train (TT) format provides efficient basic linear algebra routines for low-rank representations of tensors. In this paper, we develop interpolation formulas for a semi-Lagrangian solver in TT format. In order to efficiently implement the method, we propose a compression of the matrix representing the interpolation step and an efficient implementation of the Hadamard product. We show numerical simulations for standard test cases in two, four and six dimensional phase space. Depending on the test case, the memory requirements reduce by a factor 10^2−10^3 in four and a factor 10^5−10^6 in six dimensions compared to the full-grid method.

A low-rank projector-splitting integrator for the Vlasov--Poisson equation. Many problems encountered in plasma physics require a description by kinetic equations, which are posed in an up to six-dimensional phase space. A direct discretization of this phase space, often called the Eulerian approach, has many advantages but is extremely expensive from a computational point of view. In the present paper we propose a dynamical low-rank approximation to the Vlasov--Poisson equation, with time integration by a particular splitting method. This approximation is derived by constraining the dynamics to a manifold of low-rank functions via a tangent space projection and by splitting this projection into the subprojections from which it is built. This reduces a time step for the six- (or four-) dimensional Vlasov--Poisson equation to solving two systems of three- (or two-) dimensional advection equations over the time step, once in the position variables and once in the velocity variables, where the size of each system of advection equations is equal to the chosen rank. By a hierarchical dynamical low-rank approximation, a time step for the Vlasov--Poisson equation can be further reduced to a set of six (or four) systems of one-dimensional advection equations, where the size of each system of advection equations is still equal to the rank. The resulting systems of advection equations can then be solved by standard techniques such as semi-Lagrangian or spectral methods. Numerical simulations in two and four dimensions for linear Landau damping, for a two-stream instability and for a plasma echo problem highlight the favorable behavior of this numerical method and show that the proposed algorithm is able to drastically reduce the required computational effort.

Cross Tensor Approximation Methods for Compression and Dimensionality Reduction. Cross Tensor Approximation (CTA) is a generalization of Cross/skeleton matrix and CUR Matrix Approximation (CMA) and is a suitable tool for fast low-rank tensor approximation. It facilitates interpreting the underlying data tensors and decomposing/compressing tensors so that their structures, such as nonnegativity, smoothness, or sparsity, can be potentially preserved. This paper reviews and extends state-of-the-art deterministic and randomized algorithms for CTA with intuitive graphical illustrations. We discuss several possible generalizations of the CMA to tensors, including CTAs: based on  fiber selection, slice-tube selection, and lateral-horizontal slice selection. The main focus is on the CTA algorithms using Tucker and tubal SVD (t-SVD) models while we provide references to other decompositions such as Tensor Train (TT), Hierarchical Tucker (HT), and Canonical Polyadic (CP) decompositions. We evaluate the performance of the CTA algorithms by extensive computer simulations to compress color and medical images and compare their performance. 

A DEIM Induced CUR Factorization. We derive a CUR approximate matrix factorization based on the Discrete Empirical Interpolation Method (DEIM). For a given matrix A, such a factorization provides a low rank approximate decomposition of the form A=CUR, where C and R are subsets of the columns and rows of A, and U is constructed to make CUR a good approximation. Given a low-rank singular value decomposition A=VSW^T, the DEIM procedure uses V and W to select the columns and rows of A that form C and R. Through an error analysis applicable to a general class of CUR factorizations, we show that the accuracy tracks the optimal approximation error within a factor that depends on the conditioning of submatrices of V and W. For very large problems, V and W
 an be approximated well using an incremental QR algorithm that makes only one pass through A. Numerical examples illustrate the favorable performance of the DEIM-CUR method compared to CUR approximations based on leverage scores.

Numerical solution of the Boltzmann equation with S-model collision integral using tensor decompositions. The paper presents a new solver for the numerical solution of the Boltzmann kinetic equation with the Shakhov model collision integral (S-model) for arbitrary spatial domains. The numerical method utilizes the Tucker decomposition, which reduces the required computer memory for up to 100 times, even on a moderate velocity grid. This improvement is achieved by representing the distribution function values on a structured velocity grid as a 3D tensor in the Tucker format. The resulting numerical method makes it possible to solve complex 3D problems on modern desktop computers. Our implementation may serve as a prototype code for researchers concerned with the numerical solution of kinetic equations in 3D domains using a discrete velocity method. 